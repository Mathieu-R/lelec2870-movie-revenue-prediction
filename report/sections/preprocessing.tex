\section{Exploratory data analysis and preprocessing}

The first part of this project was to perform some data analysis to understand better our dataset and then preprocess it in order to enhance prediction results.

Our dataset does not contain that much features at a first glance. However, the principal difficultie was to manage the so-called \textbf{embeddings} that consist, for every observation, in large vectors of features (represented as float numbers). With \textit{img\_embeddings} feature being a vector of size $2048$ and \textit{text\_embeddings} a vector of size $768$. Keeping all these features would give us a very high dimensional dataset with a number of features greater than the number of observations. This situation should be avoided as it is prone to overfitting\footnote{Situation where the model learned results of training data instead of generalizing and therefore cannot predict any new data.}. 

The first thing was to remove obvious unnecessary features like \textit{title} which contains only distincts features or \textit{img\_url} and \textit{description} that we judged unnecessary as we already have the embeddings. We also noticed there were duplicated observations and removed them.

We then took a look at the missing values. There were only $4$ values missing for the column \textit{genre} consisting in only a tiny portion both datasets (\textit{X1.csv} and \textit{X2.csv}) therefore we simply dropped the observations with missing genres. However, there were respectively $264$ and $106$ values missing for the column \textit{runtime} in \textit{X1.csv} and \textit{X2.csv}. These counting for more than $5\%$ of our dataset, we did not take the risk of removing the observations containing these to avoid introducing a bias in our datasets. Looking at the dataset, it seemed like these values were missing randomly. We noticed that the shape of the distribution of the \textit{runtime} feature is not too far from a Normal. As a consequence, we decided to impute the missing values with the mean of this feature.

Looking at the distributions of the other variables, we noticed that \textit{n\_votes} and \textit{revenues} were heavily right skewed. Because of that, simply removing outliers outside $1.5 * IQR$\footnote{Interquartile range.} could lead to removing a lot of datas. To fix the skewness, we took the log of these variables resulting in a much more homogenous distribution. We also noticed that the variable \textit{is\_mature} only contained a unique value so we deleted this column.

The \textit{genres} feature consists for every observation in a list of the maximum $3$ genres that most accurately represent the given movie. Because there were not that much unique genres, we decided to \textbf{one-hot encode} them. However, the \textit{studio} feature consist in more than $300$ unique studios so we did not one-hot encode them to avoid exploding the dimension of our dataset. We choosed to \textbf{count-encode} them. The idea behind count-encoding is to replace each value by the number of times it appears in the column.

To manage the embeddings, we decided to run a PCA against them and to keep a certain amount of variance. For example, keeping $80\%$ reduced the dimension of the \textit{img\_embeddings} from $2048$ to $125$ features and of the \textit{text\_embeddings} from $768$ to $3$. We also tried to not to run the PCA but the result of our machine learning models were worse because of overfitting.

Finally, we standardized our dataset to put every features on the same scale and to have better results with the linear regression and multi-layer perceptron models (MLP). For example, in the MLP, it allows the gradient descent to converge more quickly. We ensured to only compute the transformation on the training set ...