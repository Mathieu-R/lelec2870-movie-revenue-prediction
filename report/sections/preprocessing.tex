\section{Exploratory data analysis and preprocessing}

The first part of this project was to perform some data analysis to understand better our dataset and then preprocess it in order to enhance prediction results.

Our dataset does not contain that much features at a first glance. However, the principal difficultie was to manage the so-called \textbf{embeddings} that consist, for every observation, in large vectors of features (represented as float numbers). With \textit{img\_embeddings} feature being a vector of size $2048$ and \textit{text\_embeddings} a vector of size $768$. Keeping all these features would give us a very high dimensional dataset with a number of features greater than the number of observations. This situation should be avoided as it is prone to overfitting\footnote{Situation where the model learned results of training data instead of generalizing and therefore cannot predict any new data.}. 

The first thing was to remove obvious unnecessary features like \textit{title} which contains only distincts features or \textit{img\_url} and \textit{description} that we judged unnecessary as we already have the embeddings. We also noticed there were duplicated observations and removed them.

We then took a look at the missing values. There were only $4$ values missing for the column \textit{genre} consisting in only a tiny portion both datasets (\textit{X1.csv} and \textit{X2.csv}) therefore we simply dropped the observations with missing genres. However, there were respectively $264$ and $106$ values missing for the column \textit{runtime} in \textit{X1.csv} and \textit{X2.csv}. These counting for more than $5\%$ of our dataset, we did not take the risk of removing the observations containing these to avoid introducing a bias in our datasets. Looking at the dataset, it seemed like these values were missing randomly. We noticed that the shape of the distribution of the \textit{runtime} feature is not too far from a Normal. As a consequence, we decided to impute the missing values with the mean of this feature.

Looking at the distributions of the other variables, we noticed that \textit{n\_votes} and \textit{revenues} were heavily right skewed. Because of that, simply removing outliers outside $1.5 * IQR$\footnote{Interquartile range.} could lead to removing a lot of datas. To fix the skewness, we took the log of these variables resulting in a much more homogenous distribution. We also noticed that the variable \textit{is\_mature} only contained a unique value so we deleted this column. Then, we removes the outliers for the \textit{runtime}, \textit{production\_year} and \textit{release\_year} features. To be more specific, because the distribution of these features were more or less normally distributed, we removed the observations having a z-score\footnote{Number of standard deviation with respect to the mean of the distribution.} above the range $[-3, 3]$. We only did that for the training set so that the testing set stays representative of the reality. 

The \textit{genres} feature consists for every observation in a list of the maximum $3$ genres that most accurately represent the given movie. Because there were not that much unique genres, we decided to \textbf{one-hot encode} them. However, the \textit{studio} feature suffer of high-cardinality. Indeed, this feature consists in more than $300$ unique studios so to avoid exploding the dimension of our dataset (and therefore avoid the curse of dimensionality), we replaced the less representative studios (the ones that represent less than $1\%$ of the dataset) by a special name: "other". 

To manage the embeddings, we tried to run a PCA and Kernel PCA (with a radial based function kernel) against them to keep the most significant features (those that explain the most the variance of the data points). The later has the advantage of recognizing non-linear pattern and gave slightly better results for our model. At first, we wanted to keep a certain amount of variance but we noticed we got better results for our model by being more drastic. We then chose to keep only the five most important principal components for each of the embeddings.

Finally, we standardized our dataset to put every features on the same scale and to have better results with the linear regression and multi-layer perceptron models (MLP). For example, in the MLP, it allows the gradient descent to converge more quickly. We ensured to only compute the transformation on the training set and applying it on the testing set to avoid data leakage.
We end up with a dataset of $62$ features.