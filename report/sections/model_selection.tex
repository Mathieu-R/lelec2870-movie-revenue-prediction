\section{Model selection}

We tried severall models: linear regression, K-nearest neightbors (KNN), multi-layer perceptron and a random forest. For each model except the linear regression, we choosed to tweak severall well chosen hyperparameters and used a searching algorithm along with cross-validation (5 folds) to find the best hyperparameters for each model. We used the well known \textbf{Grid Search} for most of the models. There exists other searching algorithms like \textbf{Random Search} and \textbf{Bayesian Search}. These last two have lower computational cost due to the fact they make less iterations (the number of iterations is chosen by the user) while finding a good model. However we prefered to rely on them only for very computational intensive model and/or model that required searching over a huge hyperparameters space like Random Forest.

For each model, we ran a $5$-fold cross-validation to avoid overfitting and we computed the validation score as well as the test score that's been computed on a training part of \textit{X1} that has obviously not been used to train any of our models.

\subsection{Linear regression}

The linear regression has no hyperparameters to tune so we just ran a cross-validation for each set of features we decided to keep. The results show us that the best linear model is the one with the $\textbf{35}$ best features selected.

\begin{table}[H]
	\centering
	\begin{tabular}{ccc} \toprule
	  & \multicolumn {2}{c}{scores} \\\cmidrule(lr) {2-3}
	  \#features         & validation rmse (\$)             & test rmse (\$) \\\hline
	  $5$        		& $1.629e08$                      	& $1.154e08$ \\
	  $10$        		& $9.760e07$                        & $1.029e08$ \\
	  $15$        		& $8.146e07$                        & $9.603e07$ \\
	  $20$             	& $6.848e07$                        & $8.768e07$ \\
	  $25$             	& $6.936e07$                        & $8.719e07$ \\
	  $30$     			& $6.844e07$              		    & $8.576e07$ \\
	  $\textbf{35}$     & $\textbf{6.869e07}$               & $\textbf{8.564e07}$ \\
	  $40$             	& $7.006e07$                        & $8.672e07$ \\
	  \\\hline
	\end{tabular}
	\caption{Cross-validation results of linear regression model}
	\label{tab:linear-regression-results}
\end{table}

\subsection{K-Nearest Neighbors}

For the K-Nearest Neighbors model, we need to take care of adjusting the number of neighbors hyperparameter. Indeed, a value too low will induce overfitting and a value too high will induce underfitting. Moreover, because this algorithm suffers a lot from the curse of dimensionnality, we ran it with a smaller subset of features: $[3,5,7,9,11,13,15]$. We grid searched for a number of neighbors between $1$ and $50$ first but as you can see on the figure below, this model overfits with a number of neighbors approximatively less than $7$. Then, its performance slightly decrease as this number increases.

\begin{figure}[H]
	\centering
	\includegraphics{figures/knn_eval1.pdf}
	\caption{RMSE score with respect to the number neighbors for the best K-Nearest Neighbors model}
	\label{fig:knn_eval1}
\end{figure}

We chose to run this algorithm again but this time starting from $7$ neighbors up to $50$

\begin{figure}[H]
	\centering
	\includegraphics{figures/knn_eval2.pdf}
	\caption{RMSE score with respect to the number neighbors for the best K-Nearest Neighbors model}
	\label{fig:knn_eval2}
\end{figure}

You can see the results of this second pass on the following table,

\begin{table}[H]
	\centering
	\begin{tabular}{ccc} \toprule
	  & \multicolumn {2}{c}{scores} \\\cmidrule(lr) {2-3}
	  \#features         & validation rmse (\$)             & test rmse (\$) \\\hline
	  $\textbf{3}$      & $\textbf{6.614e07}$               & $\textbf{8.036e07}$ \\
	  $5$        		& $6.642e07$                        & $8.166e07$ \\
	  $7$        		& $6.556e07$                        & $8.103e07$ \\
	  $9$             	& $6.572e07$                        & $8.176e07$ \\
	  $11$             	& $6.584e07$                        & $8.292e07$ \\
	  $13$             	& $6.542e07$                        & $9.271e07$ \\
	  $15$             	& $6.586e07$                        & $8.408e07$ \\
	  \\\hline
	\end{tabular}
	\caption{Cross-validation results of K-Nearest Neighbors model}
	\label{tab:knn-results}
\end{table}

The best model is the one with the $\textbf{3}$ best features and has $8$ neighbors.

\subsection{Multi-Layer Perceptron}

Since the multi-layer perceptron has the ability to set weights to zero, we can make the hypothesis that the feature selection will have less influence. Therefore, we wanted to test that model on a greater number of features. We also tested with all the features. We tested different number of layers and neurons per layers as well as different activation functions. We can observe that two layers gives the best results and adding more layers to the network tends to decrease the model performance.

\begin{figure}[H]
	\centering
	\includegraphics{figures/mlp_eval.pdf}
	\caption{RMSE score with respect to the number of hidden layers and their sizes for the best multi-layer perceptron model}
	\label{fig:mlp_eval1}
\end{figure}

The results are summarised in the following table,
\begin{table}[H]
	\centering
	\begin{tabular}{ccc} \toprule
	  & \multicolumn {2}{c}{scores} \\\cmidrule(lr) {2-3}
	  \#features         & validation rmse (\$)             & test rmse (\$) \\\hline
	  $5$        		& $6.649e07$                      	& $8.101e07$ \\
	  $15$     			& $6.254e07$               			& $7.469e07$ \\
	  $25$        		& $6.124e07$                        & $7.491e07$ \\
	  $\textbf{35}$     & $\textbf{6.147e07}$               & $\textbf{7.385e07}$ \\
	  all             	& $6.041e07$                        & $8.124e07$ \\
	  \\\hline
	\end{tabular}
	\caption{Cross-validation results of Multi-Layer perceptron model}
	\label{tab:mlp-results}
\end{table}

The best model is the one with the $\textbf{35}$ best features and has its first layer with $100$ neurons and its second layer with $75$ neurons. It uses a $\tanh(\cdot)$ activation function and has a starting learning rate of $0.001$.

\subsection{Random Forest}

A random forest is an ensemble technique that combines multiple decision trees. As a consequence, it has a better generalization performance than a single decision tree due to randomness which decrease the model's variance. Another thing to note is that it is little sensitive to outliers. For this model, we performed a \textbf{Bayesian Search} with $30$ iterations. Maybe we didn't find the best model but it still should be a good model. Moreover, considering the number of hyperparameters combinations we wanted to try, it allowed use to win much computational time. 

\begin{figure}[H]
	\centering
	\includegraphics{figures/rf_eval.pdf}
	\caption{RMSE score with respect to the number trees for the best random forest model}
	\label{fig:rf_eval}
\end{figure}

\begin{table}[H]
	\centering
	\begin{tabular}{ccc} \toprule
	  & \multicolumn {2}{c}{scores} \\\cmidrule(lr) {2-3}
	  \#features         & validation rmse (\$)             & test rmse (\$) \\\hline
	  $5$        		& $6.465e07	$                      	& $7.943e07$ \\
	  $10$        		& $6.342e07$                        & $7.813e07$ \\
	  $15$        		& $6.316e07$                        & $7.743e07$ \\
	  $20$             	& $6.254e07$                        & $7.711e07$ \\
	  $\textbf{25}$     & $\textbf{6.283e07}$               & $\textbf{7.706e07}$ \\
	  \\\hline
	\end{tabular}
	\caption{Cross-validation results of Random Forest model}
	\label{tab:rf-results}
\end{table}

The best model is the one with the $\textbf{25}$ best features. It uses $400$ trees.