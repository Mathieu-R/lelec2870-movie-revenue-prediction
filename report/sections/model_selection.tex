\section{Model selection}

We tried severall models: linear regression, K-nearest neightbors (KNN), multi-layer perceptron and a random forest. For each model except the linear regression, we choosed to tweak severall well chosen hyperparameters and used a searching algorithm along with cross-validation (5 folds) to find the best hyperparameters for each model. A common searching algorithm is \textbf{Grid Search} that will test every possible combination of hyperparameters. However, it can be computationaly intensive if the search space is wide which is often the case since we don't want to miss the optimal parameters. Another approach is to use \textbf{Random Search}. With this approach, we can also draw hyperparameter configurations from distributions besides discrete sets and the random search will randomly select a combination of hyperparameters among the ones defined instead of doing an exhaustive search. It allows to explore a wide range of hyperparameter configurations with time-efficiency. Looking at the result of the random search, we can identify the area where the results are promising, tighten our ranges and perform a random search again. Although this algorithm being less computationaly intensive, it can be time consuming of running and running again the random search to find the best set of hyperparameters. 

\begin{figure}[H]
	\centering
	\includegraphics{figures/grid_search_vs_random_search.png}
	\caption{Comparison of grid search and randomized for sampling $9$ different hyperparameter configurations}
	\label{fig:gridsearch_vs_randomsearch}
\end{figure}

A third approach and the one we tried in this project is to perform a \textbf{Bayesian Search} which is basically similar to random search but modelise the hyperparameters space probabilisticly to optimize the research of the best set of hyperparameters. For each model, we run a 5-fold cross-validation to avoid overfitting and we compute the validation score as well as the test score that's been computed on a part of the $X_1$ dataset that has not been used to train any of our models.

\subsection{Linear regression}

The linear regression has no hyperparameters to tune so we just ran a cross-validation for each set of features we decided to keep. The results show us that ... 

% TODO: show table

\subsection{K-Nearest Neighbors}

For the K-Nearest Neighbors model, we need to take care of adjusting the number of neighbors hyperparameter. Indeed, a value too low will induce overfitting and a value too high will induce underfitting. 

\subsection{Multi-Layer Perceptron}

\subsection{Random Forest}

A random forest is an ensemble technique that combines multiple decision trees. As a consequence, it has a better generalization performance than a single decision tree due to randomness which decrease the model's variance. Another thing to note is that it is little sensitive to outliers and do not require much hyperparameters tuning.