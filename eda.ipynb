{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look into the dataset and we could notice that the special character _\"\\N\"_ is used instead of _NaN_ for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1: entry dataset (note: X2 is the testing dataset)\n",
    "# use row `Unamed: 0` as the row index\n",
    "X1 = pd.read_csv(\"datasets/X1.csv\", na_values=\"\\\\N\")\n",
    "X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1.loc[X1[\"title\"] == \"Clown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs dataset has dimension (3540, 14)\n",
    "\n",
    "One first thing we can notice is that our dataset use a special character \"\\\\N\" for empty values. We should modify them to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y1: target dataset\n",
    "Y1 = pd.read_csv(\"datasets/Y1.csv\", header=None, names=[\"revenues\"])\n",
    "Y1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target dataset has dimension (3540, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2: testing entry dataset\n",
    "X2 = pd.read_csv(\"datasets/X2.csv\", na_values=\"\\\\N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X1 dataset contains {X1.shape[0]} observations and {X1.shape[1]} features\")\n",
    "print(f\"X2 dataset (for prediction only) contains {X2.shape[0]} observations\")\n",
    "\n",
    "print(f\"features: {list(X1.columns)}\")\n",
    "print(f\"target: {list(Y1.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "\n",
    "inputs (X1):     \n",
    "- `title`: title of the movie.    \n",
    "- `ratings`: rating on IMDB.    \n",
    "- `n_votes`: number of votes that are averaged for the given rating.    \n",
    "- `is_adult`: is the movie destined for a mature audience (0 or 1).    \n",
    "- `production_year`: the year the movie was produced.    \n",
    "- `release_year`: the year the movie was released.    \n",
    "- `runtime`: how long the movie lasts for (in minutes).    \n",
    "- `genres`: a list of maximum 3 genres that fits the movie.   \n",
    "- `studio`: the movie studio that produced the movie.        \n",
    "- `img.url`: the url of the poster of the movie.    \n",
    "- `img.embeddings`: vector of size 2048 representing the poster.    \n",
    "- `description`: synopsis of the movie.    \n",
    "- `text.embeddings`: vector of size 768 representing the synopsis.\n",
    "\n",
    "There is also an `\"Unnamed: 0\"` column that seems to be an **id for the movie**. We can remove it.\n",
    "\n",
    "target (Y1):     \n",
    "- `revenue`: the amount in dollars the movie made in the USA.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unecessary column `Unnamed: 0`\n",
    "X1.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature engineering and the sake of simplicity, we're gonna concatenate the inputs `X1` with the target `Y1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X1, Y1], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the different types of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of variables\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265dc641",
   "metadata": {},
   "source": [
    "- `n_votes` and `release_year` are of type **float** but we could have thought they would be of type **int**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4903d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.2937e+04, 1.1000e+01, 1.3450e+03, ..., 2.2860e+03, 4.1810e+03,\n",
       "       2.7379e+04])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"n_votes\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a86ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2010., 2014., 1978., 1994., 1982., 2000., 1990., 2002., 1997.,\n",
       "       2009., 2001., 2003., 2007., 1992., 1998., 1995., 2008., 1983.,\n",
       "       2015., 1991., 2016., 2006., 1973., 2004., 2011., 1999., 1986.,\n",
       "       2005., 1996., 2013., 1993., 1989., 2012., 1987., 1988., 2017.,\n",
       "       1980., 1985., 1981., 1979., 1984., 1977., 2018., 1946., 1975.,\n",
       "       1966., 1971., 1974., 1941., 1957., 1970., 1976., 1972., 1959.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"release_year\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sure we could convert `release_year` to type **int**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicated observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have any duplicate observations (we saw before that there could be duplicated movies with different `movie_id`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(subset=df.columns.difference([\"revenues\"]))].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 432 duplicated observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there is any empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of missing values\n",
    "X2.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for modelling :\n",
    "There are 264 missing values for `runtime` feature and 4 missing values for `genres` feature.\n",
    "\n",
    "Dataset for prediction :\n",
    "There are 106 missing values for `runtime` feature and 4 missing values for `genres` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of missing values\n",
    "((df.isna().sum() / df.shape[0]) * 100).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((X2.isna().sum() / X2.shape[0]) * 100).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "sns.heatmap(df.isna(), cbar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the rows containing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule of thumb**: _if values are missing at random and percentage of observations with these missing values are less than $5\\%$. We can drop them without risking of creating bias in our dataset._\n",
    "\n",
    "We have $0.3\\%$ of entries with missing values for `genres` features. These are random missing values (no reason for these to be missing, probably forgotten) so we can definitelty drop these entries without risk of creating bias in our dataset.\n",
    "However, for the `runtime` feature, we have ~ $7\\%$ of missing values. That's a little bit much for removing all these entries even though they also seem to be random missing values.\n",
    "\n",
    "We could try to impute by mean or something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"runtime\"])\n",
    "\n",
    "print(df[\"runtime\"].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If data is missing randomly but the rows with these missing values are more than $5\\%$ of the dataset, we can use **mean** (in case feature is normally distributed) or **median** (otherwise) imputation. We can also consider **mode** imputation.\n",
    "\n",
    "However, keep in mind it affects data distribution (in particular the variance is reduced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"runtime mean: {}\".format(df[\"runtime\"].mean()))\n",
    "print(\"runtime median: {}\".format(df[\"runtime\"].median()))\n",
    "print(\"runtime mode: {}\".format(df[\"runtime\"].mode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,30))\n",
    "\n",
    "i = 1\n",
    "\n",
    "for col in df.select_dtypes(\"int\"):\n",
    "    plt.subplot(4,3,i)\n",
    "    sns.histplot(df[col])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be only _non-adult_ movies (to confirm later).\n",
    "\n",
    "Movies were mainly produced between **1990** and **2010**. We have a slightly left skewed distribution but it is more or less **normally distributed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_adult\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we **do not have any movies** for a _mature audience_. \n",
    "Therefore, we could drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,30))\n",
    "\n",
    "i = 1\n",
    "\n",
    "for col in df.select_dtypes(\"float\"):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.histplot(df[col])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ratings`, `runtime` and `release_year` features are more or less **normally distributed**.\n",
    "\n",
    "- `ratings` have a mean around **6.5**.\n",
    "\n",
    "- Most movies were released between **2005** and **2010**. \n",
    "\n",
    "- `n_votes` feature and `revenues` target are **heavily right skewed**. We will have to manage that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness and outliers analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y = df[\"n_votes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y = df[\"revenues\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y = np.log(df[\"n_votes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(np.log(df[\"n_votes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y = np.log(df[\"revenues\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(np.log(df[\"revenues\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these two variables, 50% of the data are concentrated on a small range of values. But they contain a lot of outliers until pretty high values (that's why distribution is heavily right skewed).\n",
    "\n",
    "We see that we can perform a **log** transform on the 2 features to fix the skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the range of the different numericals features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.select_dtypes(include=[\"int64\", \"float64\"]).max() - df.select_dtypes(include=[\"int64\", \"float64\"]).min()).round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we count the number of unique values for the 2 categorical features `genres` and `studio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"genres\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`genre` feature contains list of maximum 3 most representative genres for each movies so there are many differents list of genres which does not mean there a as much different genres. We should preprocess them before then count how many different genres there are. \n",
    "However, we can expect there shouldn't be too many differents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"studio\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are $498$ different studios (**high-cardinality** problem), therefore, it will result in a lof of features if we One-Hot encode them. As a consequence, we would explose the dimensionnality of the datas and there would be more risk to overfit (curse of dimensionnality). Better to Label encode ? Let's check first the distribution of this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(df[\"studio\"]).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some studio that only appear one time. We could definitely remove them and replace them by a category called `other`. Then One-Hot encode this feature.\n",
    "\n",
    "Or, we can also try \"Count Encoding\" that replaces each `studio` value with the number of times it appears in the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "841f872b124843c3cec9b84aa649cbdc5d28908a0e1b01e34eab5b6f0153b5f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
