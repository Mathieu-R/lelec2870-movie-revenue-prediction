{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0df188c-0709-4523-82e1-de9a4eee7dcb",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82f72d-dcb2-47e4-8600-8ee6d7af0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tikzplotlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, RandomizedSearchCV, cross_val_predict, validation_curve, learning_curve\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
    "\n",
    "from utils.preprocessing import preprocess_duplicated_and_missing, preprocess_irrelevant_features, one_hot_encode_genres_feature, one_hot_encode_studio_feature, remove_outliers, other_fixes, standardize\n",
    "from utils.feature_extraction import extract_embeddings_features, pca_on_embeddings\n",
    "from utils.feature_selection import get_mutual_information_matrix, normalize_mutual_information_matrix, select_features_MI_kbest, mrmr, select_features_RFECV\n",
    "from utils.model_selection import linreg, ModelSelection\n",
    "from utils.plots import plot_correlation_matrix, plot_mutual_information_matrix, plot_mutual_information_with_target\n",
    "\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f65eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables for model selection\n",
    "N_SPLITS = 5\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "rmse = make_scorer(mean_squared_error, greater_is_better=True, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d3fbf4",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec795d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_datasets():\n",
    "\tX1 = pd.read_csv(\"datasets/X1.csv\", na_values=\"\\\\N\")\n",
    "\tY1 = pd.read_csv(\"datasets/Y1.csv\", header=None, names=[\"revenues\"])\n",
    "\tX2 = pd.read_csv(\"datasets/X2.csv\", na_values=\"\\\\N\")\n",
    "\n",
    "\tX1.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\tX2.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\tdf = pd.concat([X1, Y1], axis = 1)\n",
    "\n",
    "\tprint(f\"X1 dataset contains {X1.shape[0]} observations and {X1.shape[1]} features\")\n",
    "\tprint(f\"X2 dataset (for prediction only) contains {X2.shape[0]} observations\")\n",
    "\n",
    "\treturn df, X2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d53fda",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, train, X2, dataset_name):\n",
    "\tprint(\"-\" * 25)\n",
    "\tprint(f\"PREPROCESSING {dataset_name}...\")\n",
    "\tprint(\"-\" * 25)\n",
    "\t# remove duplicated observations and preprocessing missing values\n",
    "\tdf = preprocess_duplicated_and_missing(df, train)\n",
    "\tX2 = preprocess_duplicated_and_missing(X2, train)\n",
    "\n",
    "\t# remove (obvious) irrelevant/redundant features\n",
    "\tdf = preprocess_irrelevant_features(df)\n",
    "\tX2 = preprocess_irrelevant_features(X2)\n",
    "\n",
    "\t# fix high-cardinality + one-hot-encode studio feature\n",
    "\tdf, X2 = one_hot_encode_studio_feature(df, X2)\n",
    "\n",
    "\t# one-hot encode genres feature\n",
    "\tdf, X2 = one_hot_encode_genres_feature(df, X2)\n",
    "\n",
    "\t# minor fixes\n",
    "\tdf = other_fixes(df)\n",
    "\tX2 = other_fixes(X2)\n",
    "\treturn df, X2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf90033",
   "metadata": {},
   "source": [
    "## Feature extraction and dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_and_split(X, Y):\n",
    "\tprint(\"-\" * 25)\n",
    "\tprint(\"REMOVING OUTLIERS AND TRAIN-TEST SPLIT...\")\n",
    "\tprint(\"-\" * 25)\n",
    "\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, Y, train_size = 0.8, test_size = 0.2, shuffle = True, random_state = 0)\n",
    "\n",
    "\tprint(f\"training dataset dimension: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "\tprint(f\"testing dataset dimension: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "\t# remove outliers only on train set\n",
    "\t# as test set should be representative of the reality\n",
    "\tX_train, y_train = remove_outliers(X_train, y_train, [\"runtime\", \"production_year\", \"release_year\"])\n",
    "\n",
    "\treturn X_train, y_train, X_test, y_test\n",
    "\n",
    "def extract_features(X_train, y_train, X_test, y_test, X2, run_pca=True, non_linear=True):\n",
    "\tprint(\"-\" * 25)\n",
    "\tprint(\"FEATURE EXTRACTION...\")\n",
    "\tprint(\"-\" * 25)\n",
    "\n",
    "\t# extract feature vectors\n",
    "\tX_train_img_embeddings = extract_embeddings_features(X_train[\"img_embeddings\"])\n",
    "\tX_test_img_embeddings = extract_embeddings_features(X_test[\"img_embeddings\"])\n",
    "\n",
    "\tX_train_text_embeddings = extract_embeddings_features(X_train[\"text_embeddings\"])\n",
    "\tX_test_text_embeddings = extract_embeddings_features(X_test[\"text_embeddings\"])\n",
    "\n",
    "\t# should also extract features for X2\n",
    "\tX2_img_embeddings = extract_embeddings_features(X2[\"img_embeddings\"])\n",
    "\tX2_text_embeddings = extract_embeddings_features(X2[\"text_embeddings\"])\n",
    "\n",
    "\tX_train_img_df, X_test_img_df, X2_img_df = pca_on_embeddings(X_train_img_embeddings, X_test_img_embeddings, X2_img_embeddings, X_train.index, X_test.index, X2.index, prefix=\"img_feature\", total_variance_explained=5, run_pca=run_pca, non_linear=non_linear)\n",
    "\n",
    "\tX_train_text_df, X_test_text_df, X2_text_df = pca_on_embeddings(X_train_text_embeddings, X_test_text_embeddings, X2_text_embeddings, X_train.index, X_test.index, X2.index, prefix=\"text_feature\", total_variance_explained=5, run_pca=run_pca, non_linear=non_linear)\n",
    "\n",
    "\t# drop unnecessary features\n",
    "\tX_train.drop([\"img_embeddings\", \"text_embeddings\"], axis=1, inplace=True)\n",
    "\tX_test.drop([\"img_embeddings\", \"text_embeddings\"], axis=1, inplace=True)\n",
    "\tX2.drop([\"img_embeddings\", \"text_embeddings\"], axis=1, inplace=True)\n",
    "\n",
    "\t# standardize other features\n",
    "\tX_train, X_test, X2, standard_scaler = standardize(X_train, X_test, X2)\n",
    "\n",
    "\tX_train = pd.concat([X_train, X_train_img_df, X_train_text_df], axis=1)\n",
    "\tX_test = pd.concat([X_test, X_test_img_df, X_test_text_df], axis=1)\n",
    "\tX2 = pd.concat([X2, X2_img_df, X2_text_df], axis=1)\n",
    "\n",
    "\n",
    "\treturn X_train, y_train, X_test, y_test, X2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03ebfc-b137-4e30-a1a8-4621093f9a59",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c339f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, X2 = read_datasets()\n",
    "\n",
    "train_set = df\n",
    "\n",
    "# preprocessing \n",
    "df, X2 = preprocess(df, train_set, X2, \"modeling and prediction datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630abd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting input and target\n",
    "X = df.drop(\"revenues\", axis=1)\n",
    "Y = df[\"revenues\"]\n",
    "\n",
    "# remove outliers and train-test split\n",
    "X_train, y_train, X_test, y_test = remove_outliers_and_split(X, Y)\n",
    "\n",
    "# extract features + standardize (and pca)\n",
    "X_train, y_train, X_test, y_test, X2 = extract_features(X_train, y_train, X_test, y_test, X2, run_pca=True, non_linear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cdd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_candidates = [5, 10, 15, 20]\n",
    "\n",
    "def tune_model(model, search_type):\n",
    "\tbest_estimators = []\n",
    "\tbest_hyperparameters_arr = []\n",
    "\tbest_val_scores = []\n",
    "\tbest_iter_indexes = []\n",
    "\tbest_test_scores = []\n",
    "\tcv_results_arr = []\n",
    "\n",
    "\tfor k in k_candidates:\n",
    "\t\tX_train_filtered, X_test_filtered = select_features_MI_kbest(X_train, y_train, X_test, k=k)\n",
    "\n",
    "\t\tms = ModelSelection(\n",
    "\t\t\tX_train=X_train_filtered,\n",
    "\t\t\ty_train=y_train,\n",
    "\t\t\tX_test=X_test_filtered,\n",
    "\t\t\ty_test=y_test,\n",
    "\t\t\tkf=kf,\n",
    "\t\t\tscorer=rmse\n",
    "\t\t)\n",
    "\n",
    "\t\tsearcher = ms.perform_search(\n",
    "\t\t\tmodel=model,\n",
    "\t\t\tsearch_type=search_type\n",
    "\t\t)\n",
    "\n",
    "\t\t# best model among the \"n_iter\" models tested\n",
    "\t\tbest_estimator = searcher.best_estimator_\n",
    "\t\t# hyperparameters of the best model\n",
    "\t\tbest_hyperparameters = searcher.best_params_\n",
    "\t\t# mean rmse score on the validation sets using the best model\n",
    "\t\tbest_val_score = np.round(searcher.best_score_, 3)\n",
    "\t\t# index of the arrays (in searcher.cv_results_) which correspond to the best model\n",
    "\t\tbest_iter_index = searcher.best_index_\n",
    "\n",
    "\t\t# rmse score on the test set using the best model\n",
    "\t\tbest_test_score = np.round(searcher.score(X_test_filtered, y_test), 3)\n",
    "\n",
    "\t\tcv_results = searcher.cv_results_\n",
    "\n",
    "\t\tbest_estimators.append(best_estimator)\n",
    "\t\tbest_hyperparameters_arr.append(best_hyperparameters)\n",
    "\t\tbest_val_scores.append(best_val_score)\n",
    "\t\tbest_iter_indexes.append(best_iter_index)\n",
    "\t\tbest_test_scores.append(best_test_score)\n",
    "\t\tcv_results_arr.append(cv_results)\n",
    "\n",
    "\tresults = {\n",
    "\t\t\"features_kepts\": k_candidates,\n",
    "\t\t\"best_estimators\": best_estimators,\n",
    "\t\t\"hyperparameters\": best_hyperparameters_arr,\n",
    "\t\t\"val_scores\": best_val_scores,\n",
    "\t\t\"test_scores\": best_test_scores,\n",
    "\t\t\"best_iter_indexes\": best_iter_indexes,\n",
    "\t\t\"cv_results\": cv_results_arr\n",
    "\t}\n",
    "\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3db0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, results):\n",
    "\t# get the index of the best model \n",
    "\t# among the set of best models found through bayesian search\n",
    "\t# for a different number of features\n",
    "\tbest_model_idx = np.argmin(results[\"test_scores\"])\n",
    "\n",
    "\tcv_results = results[\"cv_results\"][best_model_idx]\n",
    "\tval_score = results[\"val_scores\"][best_model_idx]\n",
    "\ttest_score = results[\"test_scores\"][best_model_idx]\n",
    "\tbest_iter_index = results[\"best_iter_indexes\"][best_model_idx]\n",
    "\tbest_number_of_features = results[\"features_kepts\"][best_model_idx]\n",
    "\n",
    "\tmodel_name = model[\"name\"]\n",
    "\tparam_name = model[\"validation_param\"]\n",
    "\n",
    "\tevaluation_plot(cv_results, val_score, test_score, best_iter_index, param_name, title=f\"{model_name} - {best_number_of_features} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the best model from a bayesian search\n",
    "def evaluation_plot(cv_results, val_score, test_score, best_iter_index, param_name, title):\n",
    "\tplt.figure(figsize=(13,13))\n",
    "\tax = plt.gca()\n",
    "\n",
    "\tX_axis = np.array(cv_results[\"param_\" + param_name].data, dtype=float)\n",
    "\n",
    "\tidx_sorted = np.argsort(X_axis)\n",
    "\n",
    "\tscore = \"rmse\"\n",
    "\tcolor = \"k\"\n",
    "\n",
    "\tprint(cv_results[\"mean_train_score\"])\n",
    "\n",
    "\t# in my understanding, \"test\" is the validation set\n",
    "\tfor sample, style in ((\"train\", \"--\"), (\"test\", \"-\")):\n",
    "\t\tsample_score_mean = cv_results[f\"mean_{sample}_score\"][idx_sorted]\n",
    "\t\tsample_score_std = cv_results[f\"std_{sample}_score\"][idx_sorted]\n",
    "\n",
    "\t\tax.fill_between(\n",
    "\t\t\tX_axis[idx_sorted],\n",
    "\t\t\tsample_score_mean - sample_score_std,\n",
    "\t\t\tsample_score_mean + sample_score_std,\n",
    "\t\t\tcolor = color,\n",
    "\t\t\talpha = 0.1 if sample == \"test\" else 0\n",
    "\t\t)\n",
    "\n",
    "\t\tax.plot(\n",
    "\t\t\tX_axis[idx_sorted],\n",
    "\t\t\tsample_score_mean,\n",
    "\t\t\tstyle,\n",
    "\t\t\tcolor = color,\n",
    "\t\t\talpha = 0.1 if sample == \"test\" else 0.7,\n",
    "\t\t\tlabel = f\"{score} ({'validation' if sample == 'test' else sample})\"\n",
    "\t\t)\n",
    "\n",
    "\t\tax.plot(\n",
    "\t\t\t[X_axis[best_iter_index], ] * 2,\n",
    "\t\t\t[0, val_score],\n",
    "\t\t\tlinestyle = \"-.\",\n",
    "\t\t\tmarker = \"x\",\n",
    "\t\t\tmarkeredgewidth = 3,\n",
    "\t\t\tcolor = color,\n",
    "\t\t\tms = 8\n",
    "\t\t)\n",
    "\n",
    "\t\t# Annotate the best score for that scorer\n",
    "\t\tax.annotate(\"%0.2f\" % val_score, (X_axis[best_iter_index], val_score + 0.025))\n",
    "\n",
    "\tax.plot(\n",
    "\t\t[X_axis[idx_sorted][0], X_axis[idx_sorted][-1]], \n",
    "\t\t[test_score, test_score], \n",
    "\t\tlinestyle = 'dotted', \n",
    "\t\tmarkeredgewidth = 3, \n",
    "\t\tcolor = 'g', \n",
    "\t\tms=8, \n",
    "\t\tlabel=f\"best score on test set : {test_score}\" \n",
    "\t)\n",
    "\n",
    "\tplt.xlabel(param_name)\n",
    "\tplt.ylabel(\"RMSE\")\n",
    "\tplt.title(title, fontsize=16)\n",
    "\n",
    "\tplt.legend(loc=\"best\")\n",
    "\tplt.grid(False)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c2ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores, columns = mrmr(X_train, y_train)\n",
    "\n",
    "# scores_df = pd.Series(scores, index=columns)\n",
    "# scores_df.plot.bar(figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a798fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_mutual_information_with_target(X_train, y_train)\n",
    "# tikzplotlib.save(\"report/figures/MI_with_target.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921645f9",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"+\" * 25)\n",
    "print(\"Linear Regression\")\n",
    "print(\"+\" * 25)\n",
    "\n",
    "for k in k_candidates:\n",
    "\tX_train_filtered, X_test_filtered = select_features_MI_kbest(X_train, y_train, X_test, k=k)\n",
    "\n",
    "\tval_score, rmse_score, r2 = linreg(X_train_filtered, y_train, X_test_filtered, y_test, kf, rmse)\n",
    "\tprint(f\"val rmse: {round(val_score, 3)}\")\n",
    "\tprint(f\"train rmse: {round(rmse_score, 3)}\")\n",
    "\tprint(f\"train r2: {round(r2, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844385c0",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab82d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"+\" * 25)\n",
    "print(\"K-Nearest Neighbors\")\n",
    "print(\"+\" * 25)\n",
    "\n",
    "KNN_pipe = Pipeline([\n",
    "\t(\"model\", TransformedTargetRegressor(regressor=KNeighborsRegressor(), func=np.log1p, inverse_func=np.expm1))\n",
    "])\n",
    "\n",
    "KNN = {\n",
    "\t\"name\": \"K-Nearest Neighbors\",\n",
    "\t\"instance\": KNN_pipe,\n",
    "\t\"hyperparameters\": {\n",
    "\t\t\"model__regressor__n_neighbors\": (1, 20),\n",
    "\t\t\"model__regressor__leaf_size\": (30, 80),\n",
    "\t\t\"model__regressor__p\": [1, 2],\n",
    "\t\t\"model__regressor__weights\": [\"uniform\", \"distance\"]\n",
    "\t},\n",
    "\t\"n_iter\": 12,\n",
    "\t\"validation_param\": \"model__regressor__n_neighbors\"\n",
    "}\n",
    "\n",
    "results = tune_model(model=KNN, search_type=\"bs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4dcba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b227d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(KNN, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fe568",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_pipe = Pipeline([\n",
    "\t(\"model\", TransformedTargetRegressor(regressor=MLPRegressor(), func=np.log1p, inverse_func=np.expm1))\n",
    "])\n",
    "\n",
    "MLP = {\n",
    "\t\"name\": \"Multi-Layer Perceptron\",\n",
    "\t\"instance\": MLP_pipe,\n",
    "\t\"hyperparameters\": {\n",
    "\t\t\"model__regressor__hidden_layer_sizes\": [(10,),(10,10),(10,10,10),(10,10,10,10),(10,10,10,10,10)],\n",
    "\t\t\"model__regressor__activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "\t\t\"model__regressor__alpha\": [1e-01, 1e-02, 1e-03, 1e-04, 1e-05, 1e-06], # https://scikit-learn.org/stable/modules/neural_networks_supervised.html,\n",
    "\t\t\"model__regressor__max_iter\": np.arange(1000, 10000, 1000, dtype=int)\n",
    "\t},\n",
    "\t#\"n_iter\": 12,\n",
    "\t\"validation_param\": \"model__regressor__hidden_layer_sizes\"\n",
    "}\n",
    "\n",
    "# bayesian search does not support tuple for hyperparameters search\n",
    "# fallback to grid search\n",
    "results = tune_model(model=MLP, search_type=\"gs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(MLP, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08154bb5",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a685d6d",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 0 \n",
    "oof_rmse = 0\n",
    "\n",
    "pred_rf_test = np.zeros(len(X2))\n",
    "\n",
    "for train_index, val_index in kf.split(X_train, y_train):\n",
    "    X_train_stra, X_val_stra = X_train.iloc[train_index, :], X_train.iloc[val_index, :]\n",
    "    y_train_stra, y_val_stra = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    print()\n",
    "    print(f\"Fold: {n_fold}\")\n",
    "    print()\n",
    "    \n",
    "    # setting up a basic random forest\n",
    "    random_forest_pipe = Pipeline([\n",
    "\t\t(\"model\", TransformedTargetRegressor(regressor=RandomForestRegressor(random_state=42, n_estimators=100), func=np.log1p, inverse_func=np.expm1))\n",
    "\t])\n",
    "    \n",
    "    # train the model on the stratified k-fold training set\n",
    "    random_forest_pipe.fit(X_train_stra, y_train_stra)\n",
    "    \n",
    "    # predict regression on the whole test set\n",
    "    pred = random_forest_pipe.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true=y_test, y_pred=pred))\n",
    "    print(f\"Fold {n_fold} -- test RMSE: {rmse}\")\n",
    "    \n",
    "    n_fold += 1\n",
    "    oof_rmse += rmse\n",
    "    \n",
    "    pred_rf_test += random_forest_pipe.predict(X2[X_train.columns]) / N_SPLITS\n",
    "  \n",
    "print(f\"Out-of-fold RMSE: {oof_rmse / N_SPLITS}\")\n",
    "\n",
    "fig, ax = plt.subplots(2, figsize=(12, 7))\n",
    "sns.set(rc={\"figure.figsize\": (9, 14)})\n",
    "\n",
    "sns.distplot(y_train, ax=ax[0])\n",
    "sns.distplot(pred_rf_test, ax=ax[1])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d634c505",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b049053",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_pipe = Pipeline([\n",
    "\t(\"model\", TransformedTargetRegressor(regressor=RandomForestRegressor(random_state=42), func=np.log1p, inverse_func=np.expm1))\n",
    "])\n",
    "\n",
    "rf = {\n",
    "\t\"instance\": random_forest_pipe,\n",
    "\t\"hyperparameters\": {\n",
    "\t\t\"model__regressor__n_estimators\": (35, 100),\n",
    "\t\t\"model__regressor__criterion\": [\"absolute_error\"], #[\"squared_error\", \"absolute_error\", \"poisson\"],\n",
    "\t\t\"model__regressor__max_depth\": (7, 12), #[3, 5, 7, 10, 12, None] # none means unbounded max depth\n",
    "\t\t\"model__regressor__min_samples_split\": (2, 20),\n",
    "    \t\"model__regressor__min_samples_leaf\": (1, 20)\n",
    "\t},\n",
    "\t\"n_iter\": 12,\n",
    "\t\"validation_param\": \"model__regressor__max_depth\"\n",
    "}\n",
    "\n",
    "results = tune_model(model=rf, search_type=\"bs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe77298",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb9791-4327-4477-b427-a151438c80ad",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We're gonna make prediction about the revenue of movies present in `X2.csv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "6743b992f85ac117cab049ed48a632619826b8873c950e8ced79a0748606aab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
