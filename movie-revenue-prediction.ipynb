{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0df188c-0709-4523-82e1-de9a4eee7dcb",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82f72d-dcb2-47e4-8600-8ee6d7af0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ee64f-1b5c-4588-a576-101d1ae8de6f",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a369e-8919-43f2-90f3-61a04eaf5975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X1: entry dataset (note: X2 is the testing dataset)\n",
    "# use row `Unamed: 0` as the row index\n",
    "X1 = pd.read_csv(\"datasets/X1.csv\")\n",
    "\n",
    "print(X1.shape)\n",
    "X1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6721560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X1.loc[X1[\"title\"] == \"Clown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d6013",
   "metadata": {},
   "source": [
    "inputs dataset has dimension (3540, 14)\n",
    "\n",
    "One first thing we can notice is that our dataset use a special character \"\\\\N\" for empty values. We should modify them to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cb088e-896b-4a02-82d3-e340c866c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y1: target dataset\n",
    "Y1 = pd.read_csv(\"datasets/Y1.csv\", header = None, names = [\"revenues\"])\n",
    "\n",
    "print(Y1.shape)\n",
    "Y1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a85657",
   "metadata": {},
   "source": [
    "target dataset has dimension (3540, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f9acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2: testing entry dataset\n",
    "X2 = pd.read_csv(\"datasets/X2.csv\")\n",
    "\n",
    "print(X2.shape)\n",
    "X2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea64ae5",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "\n",
    "inputs (X1):     \n",
    "- `title`: title of the movie.    \n",
    "- `ratings`: rating on IMDB.    \n",
    "- `n_votes`: number of votes that are averaged for the given rating.    \n",
    "- `is_adult`: is the movie destined for a mature audience (0 or 1).    \n",
    "- `production_year`: the year the movie was produced.    \n",
    "- `release_year`: the year the movie was released.    \n",
    "- `runtime`: how long the movie lasts for.    \n",
    "- `genres`: a list of maximum 3 genres that fits the movie.   \n",
    "- `studio`: the movie studio that produced the movie.        \n",
    "- `img.url`: the url of the poster of the movie.    \n",
    "- `img.embeddings`: vector of size 2048 representing the poster.    \n",
    "- `description`: synopsis of the movie.    \n",
    "- `text.embeddings`: vector of size 768 representing the synopsis.\n",
    "\n",
    "There is also an `\"Unnamed: 0\"` column that seems to be an **id for the movie**. We can remove it.\n",
    "\n",
    "target (Y1):     \n",
    "- `revenue`: the amount in dollars the movie made in the USA.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18308a",
   "metadata": {},
   "source": [
    "## EDA and data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a4d81",
   "metadata": {},
   "source": [
    "What we're gonna do :\n",
    "- Reencode some categorical variables and integers variables differently\n",
    "- Remove useless / redondant features\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7740fef6",
   "metadata": {},
   "source": [
    "For feature engineering and the sake of simplicity, we're gonna concatenate the inputs `X1` with the target `Y1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0243acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X1, Y1], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f59bc5c",
   "metadata": {},
   "source": [
    "First, let's rename `Unnamed: 0` column to `movie_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace('Unnamed: 0','movie_id')\n",
    "df.drop(\"movie_id\", axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ae219",
   "metadata": {},
   "source": [
    "### Types of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5200c7a1",
   "metadata": {},
   "source": [
    "Let's check the different types of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of variables\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e918274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265dc641",
   "metadata": {},
   "source": [
    "Among the 8 object variables, we have the **2 vectors embeddings** as well as the `img_url` and `description` features that we could drop since we have the embeddings.\n",
    "We also have the following categorical variables: `title`, `genres` and `studio`.\n",
    "\n",
    "Finally, we have the `runtime` feature which contains `str` instead of `int`. It is because it uses \"\\\\N\" instead of NaN for missing values.\n",
    "\n",
    "We will drop `title` feature later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594f9c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop(columns=[\"img_url\", \"description\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d0165",
   "metadata": {},
   "source": [
    "converting `img_embeddings` and `text_embeddings` from **string** to **numpy array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589943c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_embeddings\"] = df[\"text_embeddings\"].apply(eval).apply(np.array)\n",
    "df[\"img_embeddings\"] = df[\"img_embeddings\"].apply(eval).apply(np.array)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1625a3",
   "metadata": {},
   "source": [
    "### Duplicated observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d30b22b",
   "metadata": {},
   "source": [
    "Let's check if we have any duplicate observations (we saw before that there could be duplicated movies with different `movie_id`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51adbb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce83047",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"title\"] == \"The Ox\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de10639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4928f",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473839e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"runtime\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1fa127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"runtime\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"genres\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ac92c",
   "metadata": {},
   "source": [
    "Let's see if there is any empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7484182",
   "metadata": {},
   "source": [
    "As we saw before, there is no empty values because this dataset uses the character \"\\\\N\" for empty values. Let's modify it to NaN.\n",
    "\n",
    "We also convert type of `runtime` feature to `int` instead of `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"\\\\N\" by \"-1\" (just for conversion to int)\n",
    "df.replace(\"\\\\N\", \"-1\", inplace=True)\n",
    "\n",
    "# convert to int\n",
    "df[\"runtime\"] = df[\"runtime\"].astype(int)\n",
    "\n",
    "# replace -1 (for column runtime) and \"-1\" (for column genres) by NaN\n",
    "df.replace(-1, np.nan, inplace=True)\n",
    "df.replace(\"-1\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ed36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"runtime\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"genres\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b831e",
   "metadata": {},
   "source": [
    "Now, we can check for any empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321cef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23f51f",
   "metadata": {},
   "source": [
    "There are 227 missing values for `runtime` feature and 3 missing values for `genres` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a71606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of missing values\n",
    "((df.isna().sum() / df.shape[0]) * 100).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add9fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "sns.heatmap(df.isna(), cbar = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a3924",
   "metadata": {},
   "source": [
    "Let's check the rows containing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a29d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77690af",
   "metadata": {},
   "source": [
    "**Rule of thumb**: _if values are missing at random and percentage of observations with these missing values are less than $5\\%$. We can drop them without risking of creating bias in our dataset._\n",
    "\n",
    "We have $0.3\\%$ of entries with missing values for `genres` features. These are random missing values (no reason for these to be missing, probably forgotten) so we can definitelty drop these entries without risk of creating bias in our dataset.\n",
    "However, for the `runtime` feature, we have ~ $7\\%$ of missing values. That's a little bit much for removing all these entries even though they also seem to be random missing values.\n",
    "\n",
    "We could try to impute by mean or something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"runtime\"])\n",
    "\n",
    "print(df[\"runtime\"].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c33d5ce",
   "metadata": {},
   "source": [
    "If data is missing randomly but the rows with these missing values are more than $5\\%$ of the dataset, we can use **mean** (in case feature is normally distributed) or **median** (otherwise) imputation. We can also consider **mode** imputation.\n",
    "\n",
    "However, keep in mind it affects data distribution (in particular the variance is reduced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498aad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN in genres column\n",
    "df.dropna(subset=[\"genres\"], axis=0, inplace=True)\n",
    "\n",
    "print(\"runtime mean: {}\".format(df[\"runtime\"].mean()))\n",
    "print(\"runtime median: {}\".format(df[\"runtime\"].median()))\n",
    "print(\"runtime mode: {}\".format(df[\"runtime\"].mode()))\n",
    "\n",
    "# Impute rows with NaN in runtime column. Replacing with mean.\n",
    "df[\"runtime\"].fillna(df[\"runtime\"].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ad8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b5fd89",
   "metadata": {},
   "source": [
    "### Analysis of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90e027",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,30))\n",
    "\n",
    "i = 1\n",
    "\n",
    "for col in df.select_dtypes(\"int\"):\n",
    "    plt.subplot(4,3,i)\n",
    "    sns.histplot(df[col])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b669d7",
   "metadata": {},
   "source": [
    "There seems to be only _non-adult_ movies (to confirm later).\n",
    "\n",
    "Movies were mainly produced between **1990** and **2010**. We have a left skewed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_adult\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4377bb",
   "metadata": {},
   "source": [
    "Indeed, we **do not have any movies** for a _mature audience_. \n",
    "Therefore, we could drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"is_adult\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a080c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,30))\n",
    "\n",
    "i = 1\n",
    "\n",
    "for col in df.select_dtypes(\"float\"):\n",
    "    plt.subplot(4,3,i)\n",
    "    sns.histplot(df[col])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642461da",
   "metadata": {},
   "source": [
    "Ratings are more or less normally distributed with a mean around **6.5**.\n",
    "\n",
    "The number of votes are pretty homogeneous amoung the movies.\n",
    "\n",
    "Most movies were released between **2005** and **2010**. We have a left skewed distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74a485",
   "metadata": {},
   "source": [
    "It doesn't seem to make sense to have `n_votes` and `release_year` as \"float\". Let's convert them into \"int\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"release_year\"] = df[\"release_year\"].astype(int)\n",
    "df[\"n_votes\"] = df[\"n_votes\"].astype(int)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d344fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542471de",
   "metadata": {},
   "source": [
    "### Skewness, outliers and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce9f6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y = df[\"n_votes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa5c295",
   "metadata": {},
   "source": [
    "We notice a big difference of scale for the `n_votes` feature. It also contains outliers. We do not remove outliers in testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee840cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y = df[\"revenues\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ffa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove outliers via IQR (because data has not normal shape)\n",
    "def find_boundaries(df, variable):\n",
    "    iqr = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n",
    "\n",
    "    lower_bound = df[variable].quantile(0.25) - (iqr * 1.5)\n",
    "    upper_bound = df[variable].quantile(0.75) + (iqr * 1.5)\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "lower_bound_votes, upper_bound_votes = find_boundaries(X1, \"n_votes\")\n",
    "\n",
    "print(lower_bound_votes)\n",
    "print(upper_bound_votes)   \n",
    "\n",
    "votes_outliers = np.where(X1[\"n_votes\"] > upper_bound_votes, True, np.where(X1[\"n_votes\"] < upper_bound_votes, True, False))\n",
    "print(votes_outliers.sum())\n",
    "\n",
    "X1_without_outliers = X1.loc[(~votes_outliers), ]\n",
    "\n",
    "print(X1.shape)\n",
    "print(X1_without_outliers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef61498",
   "metadata": {},
   "source": [
    "Then, we standardize the data. To do that, we apply a scaler on the training set and then apply it on both training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe11eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a62b4",
   "metadata": {},
   "source": [
    "Let's analyze the range of the different numericals features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=[\"int64\", \"float64\"]).max() - df.select_dtypes(include=[\"int64\", \"float64\"]).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b8324",
   "metadata": {},
   "source": [
    "Standardizing data with `Standadardizer` or `Normalizer` is not a good idea with skewed data. \n",
    "Since we cannot remove outliers of `n_votes` feature, we use `RobustScaler` that works better with skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30002a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "numerical_features = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "numerical_features = numerical_features.to_list()\n",
    "print(numerical_features)\n",
    "\n",
    "# fit the scaler on training dataset\n",
    "scaler.fit(df[numerical_features].to_numpy())\n",
    "\n",
    "# apply the scaler on both training and testing datasets\n",
    "df_scaled = scaler.transform(df[numerical_features])\n",
    "#X2_scaled = scaler.transform(X2[numerical_features])\n",
    "\n",
    "df[numerical_features] = pd.DataFrame(df_scaled, columns=numerical_features, index=df.index)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda18c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"n_votes\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e1208b",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39cae9c",
   "metadata": {},
   "source": [
    "We should then One-Hot encode the genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527dae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate all genres into one big list of list of genres\n",
    "genres_list = df[\"genres\"].str.split(\",\").tolist()\n",
    "\n",
    "unique_genres = []\n",
    "\n",
    "# retrieve each genre\n",
    "for sublist in genres_list:\n",
    "    for genre in sublist:\n",
    "        if genre not in unique_genres:\n",
    "            unique_genres.append(genre)\n",
    "\n",
    "# sort\n",
    "unique_genres = sorted(unique_genres)\n",
    "print(unique_genres)\n",
    "\n",
    "# one hot encode movies genres\n",
    "df = df.reindex(X1.columns.tolist() + unique_genres, axis = 1, fill_value = 0)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for genre in row[\"genres\"].split(\",\"):\n",
    "        df.loc[index, genre] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop old genres column\n",
    "df.drop(\"genres\", axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b11637",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1991194e",
   "metadata": {},
   "source": [
    "We should also One-Hot encode the `studio` feature. \n",
    "Well, there are a lof of different studios, therefore, it will result in a lof of features if we One-Hot encode them. As a consequence, we would explose the dimensionnality of the datas and there would be more risk to overfit. Better to Label encode ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"studio\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"studio\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be290a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df[\"studio\"] = label_encoder.fit_transform(df[\"studio\"].to_numpy())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a72a5",
   "metadata": {},
   "source": [
    "We could drop `title` feature as it doesn't have any value for the target (all unique values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ebd72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"title\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03ebfc-b137-4e30-a1a8-4621093f9a59",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We're gonna build regression models :\n",
    "- Linear regression\n",
    "- K-Nearest Neighbors \n",
    "- MLP\n",
    "- One other non-linear method (can be one not seen during the course)\n",
    "\n",
    "We're gonna do **feature selection** and **model selection**.\n",
    "/!\\ model selection can require a lot of computation time /!\\\n",
    "\n",
    "We're gonna validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37782a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression, SequentialFeatureSelector, SelectFromModel\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d8aa80",
   "metadata": {},
   "source": [
    "### Train - Test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675ea8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df.drop(\"revenues\", axis=1)\n",
    "Y1 = df[\"revenues\"]\n",
    "\n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, train_size = 0.8, test_size = 0.2, shuffle = True, random_state = 0)\n",
    "\n",
    "print(f\"training dataset dimension: X1 = {X1_train.shape}, Y1 = {Y1_train.shape}\")\n",
    "print(f\"testing dataset dimension: X1 = {X1_test.shape}, Y1 = {Y1_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5b743d",
   "metadata": {},
   "source": [
    "### Feature selection : Filter Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7788e12",
   "metadata": {},
   "source": [
    "We want to remove redundant or irrelevant features to improve computational efficiency and reduce the risk of overfitting.\n",
    "As I understand, there's multiple ways to selection features via filter method. As a reminder, filter method is independent of any machine learning model but does not take into account feature redundancy. \n",
    "\n",
    "Some of them are:\n",
    "- `Chi-Square` and `ANOVA`: for categorical variables and categorical targets    \n",
    "- `Correlation matrix`: for continuous variables, continuous target and linear model    \n",
    "- `Mutual information`: for continuous variables, continuous target and non-linear model     \n",
    "\n",
    "Since we're dealing with continuous target and we will train linear and non-linear models, we use the two last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c714cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08ac5b",
   "metadata": {},
   "source": [
    "#### Correlation matrix (linear models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize = (12,10))\n",
    "\n",
    "corr_with_target = X1.select_dtypes(\"int\", \"float\").corrwith(Y1)\n",
    "\n",
    "print(corr_with_target)\n",
    "sns.heatmap(corr_with_target, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d34005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features = corr_with_target[corr_with_target > 0.4]\n",
    "\n",
    "print(relevant_features.items())\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get relevant features list for linear models\n",
    "rf_lm = [k for k,v in relevant_features.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f232bd6",
   "metadata": {},
   "source": [
    "#### Mutual information (non-linear models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f84cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info_matrix = mutual_info_regression(X1, Y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a453dee2",
   "metadata": {},
   "source": [
    "### Feature selection: Wrapper Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "ba4fd343",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zn/_2h7w2j547357vpm3_5pxrsr0000gn/T/ipykernel_84258/3198081237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m     def __array_wrap__(\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=500, random_state=0)\n",
    "forest.fit(X1_train, Y1_train)\n",
    "\n",
    "sfm = SelectFromModel(forest, threshold=0.1, prefit=True)\n",
    "\n",
    "X1_selected = sfm.transform(X1_train)\n",
    "\n",
    "feat_labels = X1_train.columns\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "for i in range(X1_selected.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (i + 1, feat_labels[indices[i]], importances[indices[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb9791-4327-4477-b427-a151438c80ad",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We're gonna make prediction about the revenue of movies present in `X2.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2595c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "841f872b124843c3cec9b84aa649cbdc5d28908a0e1b01e34eab5b6f0153b5f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
