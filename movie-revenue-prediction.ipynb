{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0df188c-0709-4523-82e1-de9a4eee7dcb",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b82f72d-dcb2-47e4-8600-8ee6d7af0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tikzplotlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, RandomizedSearchCV, cross_val_predict, validation_curve, learning_curve\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
    "\n",
    "from utils.preprocessing import preprocess_duplicated_and_missing, preprocess_irrelevant_features, one_hot_encode_genres_feature, one_hot_encode_studio_feature, remove_outliers, other_fixes, standardize\n",
    "from utils.feature_extraction import extract_embeddings_features, pca_on_embeddings\n",
    "from utils.feature_selection import get_mutual_information_matrix, normalize_mutual_information_matrix, select_features_MI_kbest, mrmr, select_features_RFECV\n",
    "from utils.model_selection import linreg, perform_grid_search, perform_random_search, perform_halving_random_search, perform_bayesian_search, evaluate_model, validate_model, compare_models, test_model, ModelSelection\n",
    "from utils.plots import plot_correlation_matrix, plot_mutual_information_matrix, plot_mutual_information_with_target, plot_residuals, plot_predictions, validate_model_with_feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f65eec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# declare variables for model selection\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "rmse = make_scorer(mean_squared_error, greater_is_better=False, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62fc8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d3fbf4",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec795d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_datasets():\n",
    "\tX1 = pd.read_csv(\"datasets/X1.csv\", na_values=\"\\\\N\")\n",
    "\tY1 = pd.read_csv(\"datasets/Y1.csv\", header=None, names=[\"revenues\"])\n",
    "\tX2 = pd.read_csv(\"datasets/X2.csv\", na_values=\"\\\\N\")\n",
    "\n",
    "\tX1.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\tdf = pd.concat([X1, Y1], axis = 1)\n",
    "\n",
    "\tprint(f\"X1 dataset contains {X1.shape[0]} observations and {X1.shape[1]} features\")\n",
    "\tprint(f\"X2 dataset (for prediction only) contains {X2.shape[0]} observations\")\n",
    "\n",
    "\treturn df, X2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d53fda",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2711b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, train, dataset_name):\n",
    "\tprint(\"-\" * 25)\n",
    "\tprint(f\"PREPROCESSING {dataset_name}...\")\n",
    "\tprint(\"-\" * 25)\n",
    "\t# remove duplicated observations and preprocessing missing values\n",
    "\tdf = preprocess_duplicated_and_missing(df, train)\n",
    "\t# remove (obvious) irrelevant/redundant features\n",
    "\tdf = preprocess_irrelevant_features(df)\n",
    "\n",
    "\t# fix high-cardinality + one-hot-encode studio feature\n",
    "\tdf = one_hot_encode_studio_feature(df)\n",
    "\n",
    "\t# one-hot encode genres feature\n",
    "\tdf = one_hot_encode_genres_feature(df)\n",
    "\n",
    "\t# minor fixes\n",
    "\tdf = other_fixes(df)\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf90033",
   "metadata": {},
   "source": [
    "## Feature extraction and dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc1ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_and_split(X, Y):\n",
    "\tprint(\"-\" * 25)\n",
    "\tprint(\"REMOVING OUTLIERS AND TRAIN-TEST SPLIT...\")\n",
    "\tprint(\"-\" * 25)\n",
    "\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, Y, train_size = 0.8, test_size = 0.2, shuffle = True, random_state = 0)\n",
    "\n",
    "\tprint(f\"training dataset dimension: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "\tprint(f\"testing dataset dimension: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "\t# remove outliers only on train set\n",
    "\t# as test set should be representative of the reality\n",
    "\tX_train, y_train = remove_outliers(X_train, y_train, [\"runtime\", \"production_year\", \"release_year\"])\n",
    "\n",
    "\treturn X_train, y_train, X_test, y_test\n",
    "\n",
    "def extract_features(X_train, y_train, X_test, y_test, run_pca=True, non_linear=True):\n",
    "\tprint(\"-\" * 25)\n",
    "\tprint(\"FEATURE EXTRACTION...\")\n",
    "\tprint(\"-\" * 25)\n",
    "\n",
    "\t# extract feature vectors\n",
    "\tX_train_img_embeddings = extract_embeddings_features(X_train[\"img_embeddings\"])\n",
    "\tX_test_img_embeddings = extract_embeddings_features(X_test[\"img_embeddings\"])\n",
    "\n",
    "\tX_train_text_embeddings = extract_embeddings_features(X_train[\"text_embeddings\"])\n",
    "\tX_test_text_embeddings = extract_embeddings_features(X_test[\"text_embeddings\"])\n",
    "\n",
    "\tX_train_img_df, X_test_img_df = pca_on_embeddings(X_train_img_embeddings, X_test_img_embeddings, X_train.index, X_test.index, prefix=\"img_feature\", total_variance_explained=0.6, run_pca=run_pca, non_linear=non_linear)\n",
    "\n",
    "\tX_train_text_df, X_test_text_df = pca_on_embeddings(X_train_text_embeddings, X_test_text_embeddings, X_train.index, X_test.index, prefix=\"text_feature\", total_variance_explained=0.8, run_pca=run_pca, non_linear=non_linear)\n",
    "\n",
    "\t# drop unnecessary features\n",
    "\tX_train.drop([\"img_embeddings\", \"text_embeddings\"], axis=1, inplace=True)\n",
    "\tX_test.drop([\"img_embeddings\", \"text_embeddings\"], axis=1, inplace=True)\n",
    "\n",
    "\t# standardize other features\n",
    "\tX_train, X_test, standard_scaler = standardize(X_train, X_test)\n",
    "\n",
    "\tX_train = pd.concat([X_train, X_train_img_df, X_train_text_df], axis=1)\n",
    "\tX_test = pd.concat([X_test, X_test_img_df, X_test_text_df], axis=1)\n",
    "\n",
    "\t# should also extract features for X2\n",
    "\n",
    "\treturn X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03ebfc-b137-4e30-a1a8-4621093f9a59",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c339f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 dataset contains 3540 observations and 13 features\n",
      "X2 dataset (for prediction only) contains 1518 observations\n",
      "-------------------------\n",
      "PREPROCESSING modeling dataset...\n",
      "-------------------------\n",
      "[X] Removing duplicated and missing values\n",
      "[X] Removing irrelevant features\n",
      "[X] One-Hot encoding studio feature\n",
      "[X] One-Hot encoding genres feature\n",
      "[X] Minor fixes\n",
      "-------------------------\n",
      "PREPROCESSING prediction dataset...\n",
      "-------------------------\n",
      "[X] Removing duplicated and missing values\n",
      "[X] Removing irrelevant features\n",
      "[X] One-Hot encoding studio feature\n",
      "[X] One-Hot encoding genres feature\n",
      "[X] Minor fixes\n"
     ]
    }
   ],
   "source": [
    "df, X2 = read_datasets()\n",
    "\n",
    "train_set = df\n",
    "\n",
    "# preprocessing \n",
    "df = preprocess(df, train_set, \"modeling dataset\")\n",
    "X2 = preprocess(X2, train_set, \"prediction dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "630abd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "REMOVING OUTLIERS AND TRAIN-TEST SPLIT...\n",
      "-------------------------\n",
      "training dataset dimension: X_train: (2484, 54), y_train: (2484,)\n",
      "testing dataset dimension: X_test: (621, 54), y_test: (621,)\n",
      "-------------------------\n",
      "FEATURE EXTRACTION...\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69080d69ef9436bb7a2f92f8787bc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "extracting features:   0%|          | 0/2408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4774bb5c7b8149f48be91cd0c4c70806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "extracting features:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b98281dcee481cb945889f9b48aac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "extracting features:   0%|          | 0/2408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd47df2e05540cb9cf767a7f0e54853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "extracting features:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully reduced from 2048 features to 56 features keeping 60.0% of variance explained\n",
      "successfully reduced from 768 features to 3 features keeping 80.0% of variance explained\n"
     ]
    }
   ],
   "source": [
    "# spliting input and target\n",
    "X = df.drop(\"revenues\", axis=1)\n",
    "Y = df[\"revenues\"]\n",
    "\n",
    "# remove outliers and train-test split\n",
    "X_train, y_train, X_test, y_test = remove_outliers_and_split(X, Y)\n",
    "\n",
    "# extract features + standardize (and pca)\n",
    "X_train, y_train, X_test, y_test = extract_features(X_train, y_train, X_test, y_test, run_pca=True, non_linear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f185571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_filtered, X_test_filtered = select_features_RFECV(X_train, y_train, X_test, kf, rmse)\n",
    "#X_train_filtered, X_test_filtered = select_features_MI_kbest(X_train, y_train, X_test, k=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c2ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores, columns = mrmr(X_train, y_train)\n",
    "\n",
    "# scores_df = pd.Series(scores, index=columns)\n",
    "# scores_df.plot.bar(figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEPT_FEATURES = ['n_votes', 'studio_other', 'release_year', 'production_year',\n",
    "#        'Adventure', 'runtime', 'studio_Uni.', 'studio_BV', 'studio_WB',\n",
    "#        'Action', 'studio_Par.', 'studio_SPC', 'studio_Fox',\n",
    "#        'text_feature2', 'img_feature43', 'text_feature0', 'studio_Col.',\n",
    "#        'studio_MGM', 'studio_Sony', 'studio_Orion', 'studio_Strand',\n",
    "#        'studio_Magn.', 'studio_IFC', 'studio_Mira.', 'studio_Eros',\n",
    "#        'Animation', 'studio_Reg.', 'studio_Gold.', 'studio_NL', 'Comedy',\n",
    "#        'Mystery', 'Horror', 'Fantasy', 'Drama', 'studio_FoxS']\n",
    "\n",
    "# OTHER_KEPT_FEATURES = ['n_votes', 'studio_other', 'release_year', 'Adventure', 'runtime',\n",
    "#        'studio_WB', 'studio_Uni.', 'Action', 'studio_BV', 'studio_Par.',\n",
    "#        'studio_SPC', 'img_feature0', 'studio_Fox', 'studio_Sony',\n",
    "#        'studio_Col.', 'studio_MGM', 'studio_Magn.', 'studio_Strand',\n",
    "#        'studio_Orion', 'studio_Mira.', 'Comedy', 'studio_IFC', 'Family']\n",
    "\n",
    "# X_train_filtered = X_train[columns[scores > 0.001]]\n",
    "# X_test_filtered = X_test[columns[scores > 0.001]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISCRETE_FEATURES = [\"ratings\", \"production_year\", \"release_year\"]\n",
    "# STUDIOS_FEATURES = X_train.columns[X_train.columns.str.startswith('studio')].tolist()\n",
    "# GENRES_FEATURES = X_train.coumns[X_train.columns.str.startswith(\"genre\")].tolist()\n",
    "\n",
    "# DISCRETE_FEATURES = np.concatenate(DISCRETE_FEATURES, STUDIOS_FEATURES, GENRES_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a798fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_mutual_information_with_target(X_train, y_train)\n",
    "# tikzplotlib.save(\"report/figures/MI_with_target.tex\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "921645f9",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"+\" * 25)\n",
    "print(\"Linear Regression\")\n",
    "print(\"+\" * 25)\n",
    "\n",
    "val_score, rmse_score, r2 = linreg(X_train_filtered, y_train, X_test_filtered, y_test, kf, rmse)\n",
    "print(f\"val rmse: {round(val_score, 3)}\")\n",
    "print(f\"train rmse: {round(rmse_score, 3)}\")\n",
    "print(f\"train r2: {round(r2, 3)}\")\n",
    "\n",
    "val_score, rmse_score, r2 = linreg(X_train, y_train, X_test, y_test, kf, rmse)\n",
    "print(f\"val rmse: {round(val_score, 3)}\")\n",
    "print(f\"train rmse: {round(rmse_score, 3)}\")\n",
    "print(f\"train r2: {round(r2, 3)}\")\n",
    "\n",
    "# percentiles_candidates = [40, 45, 50, 55, 60]\n",
    "# val_scores = []\n",
    "# rmse_scores = []\n",
    "# f1_scores = []\n",
    "\n",
    "# # test different percentage of features to keep (MI)\n",
    "# for percentile in percentiles_candidates:\n",
    "# \tX_train_MI, X_test_MI = select_features_MI(X_train, y_train, X_test, percentile=percentile)\n",
    "# \tval_score, rmse_score = linreg(X_train_MI, y_train, X_test_MI, y_test, kf, rmse)\n",
    "# \tval_scores.append(np.round(lr_score, 3))\n",
    "# \trmse_scores.append(np.round(rmse_score, 3))\n",
    "\n",
    "# pd.DataFrame({\n",
    "# \t\"Features keps [%]\": percentiles_candidates,\n",
    "# \t\"val scores (RMSE)\": val_scores,\n",
    "# \t\"test scores (RMSE)\": rmse_scores\n",
    "# })\n",
    "\n",
    "# plt.plot(percentiles_candidates, scores, color=\"blue\", marker=\"o\")\n",
    "# plt.title(\"Linear Regression: RMSE for different percentages of feature kept (MI)\")\n",
    "# plt.xlabel(\"percentage of features kepts\")\n",
    "# plt.ylabel(\"score (RMSE)\")\n",
    "\n",
    "# compare with RFE\n",
    "#X_train_RFE, X_test_RFE = select_features_RFE(X_train, y_train, X_test)\n",
    "#lr_score = linreg(X_train_RFE, y_train, X_test_RFE, y_test, kf, rmse)\n",
    "\n",
    "#print(\"[RFE] Linear Regression RMSE: {:.3f}\".format(lr_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "844385c0",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab82d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"+\" * 25)\n",
    "print(\"K-Nearset Neighbors\")\n",
    "print(\"+\" * 25)\n",
    "\n",
    "KNN_pipe = Pipeline([\n",
    "\t(\"model\", TransformedTargetRegressor(regressor=KNeighborsRegressor(), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "KNN = {\n",
    "\t\"instance\": KNN_pipe,\n",
    "\t\"hyperparameters\": {\n",
    "\t\t\"model__regressor__n_neighbors\": np.linspace(1, 50, 10, dtype=int),\n",
    "\t\t\"model__regressor__p\": [1, 2],\n",
    "\t\t\"model__regressor__weights\": [\"uniform\", \"distance\"]\n",
    "\t},\n",
    "\t\"n_iter\": 100,\n",
    "\t\"validation_param\": \"model__regressor__n_neighbors\"\n",
    "}\n",
    "\n",
    "best_estimator, best_params, best_score = test_model(\n",
    "\tmodel=KNN, \n",
    "\tname=\"K-Nearest Neighbors\", \n",
    "\tX_train=X_train_filtered, \n",
    "\ty_train=y_train, \n",
    "\tX_test=X_test_filtered, \n",
    "\ty_test=y_test, \n",
    "\tkf=kf, \n",
    "\tscorer=rmse\n",
    ")\n",
    "\n",
    "print(best_params)\n",
    "print(f\"train rmse: {round(best_score, 3)}\")\n",
    "\n",
    "# percentiles_candidates = [40, 50, 60]\n",
    "\n",
    "# estimators = []\n",
    "# scores = []\n",
    "\n",
    "# for percentile in percentiles_candidates:\n",
    "# \tX_train_MI, X_test_MI = select_features_MI(X_train, y_train, X_test, percentile=percentile)\n",
    "\n",
    "# \tbest_estimator, best_params, best_score = test_model(\n",
    "# \t\tmodel=KNN, \n",
    "# \t\tname=\"K-Nearest Neighbors\", \n",
    "# \t\tX_train=X_train_MI, \n",
    "# \t\ty_train=y_train, \n",
    "# \t\tX_test=X_test_MI, \n",
    "# \t\ty_test=y_test, \n",
    "# \t\tkf=kf, \n",
    "# \t\tscorer=rmse\n",
    "# \t)\n",
    "\n",
    "# \testimators.append(best_estimator)\n",
    "# \tscores.append(best_score)\n",
    "\n",
    "# validate_model_with_feature_selection(percentiles_candidates, estimators, \"K-Nearest Neighbors\", KNN[\"validation_param\"], KNN[\"hyperparameters\"][KNN[\"validation_param\"]], X_train_MI, y_train, X_test_MI, y_test, kf, rmse)\n",
    "\n",
    "# pd.DataFrame({\n",
    "# \t\"Features keps [%]\": percentiles_candidates,\n",
    "# \t\"val scores\": scores\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b227d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model(best_estimator, \"K-Nearest Neighbors\", KNN[\"validation_param\"], KNN[\"hyperparameters\"][KNN[\"validation_param\"]], X_train_filtered, y_train, X_test_filtered, y_test, kf, rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "435fe568",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_pipe = Pipeline([\n",
    "\t(\"model\", TransformedTargetRegressor(regressor=MLPRegressor(), func=np.log1p, inverse_func=np.expm1))\n",
    "])\n",
    "\n",
    "MLP = {\n",
    "\t\"instance\": MLP_pipe,\n",
    "\t\"hyperparameters\": {\n",
    "\t\t\"model__regressor__hidden_layer_sizes\": [(25,25,25),(25,25),(25,)],\n",
    "\t\t\"model__regressor__activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "\t\t\"model__regressor__alpha\": 10.0 ** -np.arange(1, 7), # https://scikit-learn.org/stable/modules/neural_networks_supervised.html,\n",
    "\t\t\"model__regressor__max_iter\": [int(x) for x in np.linspace(10, 10000, 100)]\n",
    "\t},\n",
    "\t\"n_iter\": 10,\n",
    "\t\"validation_param\": \"model__regressor__hidden_layer_sizes\"\n",
    "}\n",
    "\n",
    "best_estimator, best_params, best_score = test_model(\n",
    "\tmodel=MLP, \n",
    "\tname=\"Multi-Layer Perceptron\", \n",
    "\tX_train=X_train_filtered, \n",
    "\ty_train=y_train, \n",
    "\tX_test=X_test_filtered, \n",
    "\ty_test=y_test, \n",
    "\tkf=kf, \n",
    "\tscorer=rmse\n",
    ")\n",
    "\n",
    "print(best_params)\n",
    "print(f\"train rmse: {round(best_score, 3)}\")\n",
    "\n",
    "# percentiles_candidates = [40, 50, 60]\n",
    "\n",
    "# estimators = []\n",
    "# scores = []\n",
    "\n",
    "# for percentile in percentiles_candidates:\n",
    "# \tX_train_MI, X_test_MI = select_features_MI(X_train, y_train, X_test, percentile=percentile)\n",
    "\n",
    "# \tbest_estimator, best_params, best_score = test_model(\n",
    "# \t\tmodel=MLP, \n",
    "# \t\tname=\"Multi-Layer Perceptron\", \n",
    "# \t\tX_train=X_train_MI, \n",
    "# \t\ty_train=y_train, \n",
    "# \t\tX_test=X_test_MI, \n",
    "# \t\ty_test=y_test, \n",
    "# \t\tkf=kf, \n",
    "# \t\tscorer=rmse\n",
    "# \t)\n",
    "\n",
    "# \testimators.append(best_estimator)\n",
    "# \tscores.append(best_score)\n",
    "\n",
    "# pd.DataFrame({\n",
    "# \t\"Features keps [%]\": percentiles_candidates,\n",
    "# \t\"val scores\": scores\n",
    "# })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08154bb5",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b049053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "FEATURE SELECTION (MUTUAL INFORMATION)...\n",
      "-------------------------\n",
      "reduced from 111 features to 10 features\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BayesSearchCV' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zn/_2h7w2j547357vpm3_5pxrsr0000gn/T/ipykernel_32180/2174793761.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \t)\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \tbest_estimator, best_params, best_score = ms.test_model(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrf_bayes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Random Forest\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Lab/movie-revenue-predictor/utils/model_selection.py\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(self, model, name)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m \t\tbest_estimator, best_params, best_score = perform_bayesian_search(\n\u001b[0m\u001b[1;32m    202\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"instance\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                         \u001b[0mhyperparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hyperparameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Lab/movie-revenue-predictor/utils/model_selection.py\u001b[0m in \u001b[0;36mperform_bayesian_search\u001b[0;34m(model, hyperparameters, n_iter, X_train, y_train, X_test, y_test, kf, scorer)\u001b[0m\n\u001b[1;32m     83\u001b[0m \t\t))\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mbayesian_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus_print\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbayesian_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/skopt/searchcv.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_kwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# BaseSearchCV never ranked train scores,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/skopt/searchcv.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mn_iter\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0meval_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optim_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36meval_callbacks\u001b[0;34m(callbacks, result)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mdecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdecision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Lab/movie-revenue-predictor/utils/model_selection.py\u001b[0m in \u001b[0;36mstatus_print\u001b[0;34m(optim_results)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstatus_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m# get all models tested so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mall_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbayesian_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;31m# get current parameters and the best parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BayesSearchCV' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "random_forest_pipe = Pipeline([\n",
    "\t(\"model\", TransformedTargetRegressor(regressor=RandomForestRegressor(random_state=42), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "rf = {\n",
    "\t\"instance\": random_forest_pipe,\n",
    "\t\"hyperparameters\": {\n",
    "\t\t\"model__regressor__n_estimators\": [int(x) for x in np.linspace(35, 100, 10)],\n",
    "\t\t\"model__regressor__criterion\": [\"absolute_error\"], #[\"squared_error\", \"absolute_error\", \"poisson\"],\n",
    "\t\t\"model__regressor__max_features\": [\"auto\"], #[\"auto\", \"sqrt\"],\n",
    "\t\t\"model__regressor__max_depth\": [10, 12, None] #[3, 5, 7, 10, 12, None] # none means unbounded max depth\n",
    "\t},\n",
    "\t\"n_iter\": 10,\n",
    "\t\"validation_param\": \"model__regressor__max_depth\"\n",
    "}\n",
    "\n",
    "rf_bayes = {\n",
    "\t\"instance\": random_forest_pipe,\n",
    "\t\"hyperparameters\": {\n",
    "\t\t\"model__regressor__n_estimators\": (35, 100),\n",
    "\t\t\"model__regressor__criterion\": [\"absolute_error\"], #[\"squared_error\", \"absolute_error\", \"poisson\"],\n",
    "\t\t\"model__regressor__max_features\": [\"auto\"], #[\"auto\", \"sqrt\"],\n",
    "\t\t\"model__regressor__max_depth\": (7, 12), #[3, 5, 7, 10, 12, None] # none means unbounded max depth\n",
    "\t\t\"model__regressor__min_samples_split\": (2, 20),\n",
    "    \t\"model__regressor__min_samples_leaf\": (1, 20)\n",
    "\t},\n",
    "\t\"n_iter\": 12,\n",
    "\t\"validation_param\": \"model__regressor__max_depth\"\n",
    "}\n",
    "\n",
    "# best_estimator, best_params, best_score = test_model(\n",
    "# \tmodel=rf, \n",
    "# \tname=\"Random Forest\", \n",
    "# \tX_train=X_train_filtered, \n",
    "# \ty_train=y_train, \n",
    "# \tX_test=X_test_filtered, \n",
    "# \ty_test=y_test, \n",
    "# \tkf=kf, \n",
    "# \tscorer=rmse\n",
    "# )\n",
    "\n",
    "# print(best_params)\n",
    "# print(f\"train rmse: {round(best_score, 3)}\")\n",
    "\n",
    "k_candidates = [10, 20, 30, 40, 50, 55, 60, 65, 70, 75, 80]\n",
    "\n",
    "estimators = []\n",
    "scores = []\n",
    "\n",
    "for k in k_candidates:\n",
    "\tX_train_filtered, X_test_filtered = select_features_MI_kbest(X_train, y_train, X_test, k=k)\n",
    "\n",
    "\tms = ModelSelection(\n",
    "\t\tX_train=X_train_filtered,\n",
    "\t\ty_train=y_train,\n",
    "\t\tX_test=X_test_filtered,\n",
    "\t\ty_test=y_test,\n",
    "\t\tkf=kf,\n",
    "\t\tscorer=rmse\n",
    "\t)\n",
    "\n",
    "\tbest_estimator, best_params, best_score = ms.test_model(\n",
    "\t\tmodel=rf_bayes, \n",
    "\t\tname=\"Random Forest\"\n",
    "\t)\n",
    "\n",
    "\testimators.append(best_estimator)\n",
    "\tscores.append(round(best_score, 3))\n",
    "\n",
    "results = pd.DataFrame({\n",
    "\t\"Features kepts [%]\": k_candidates,\n",
    "\t\"val scores\": scores\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7042bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot validation curve of the best model\n",
    "best_idx = np.argmin(scores)\n",
    "\n",
    "best_estimator = estimators[best_idx]\n",
    "best_number_of_faetures = k_candidates[best_idx]\n",
    "\n",
    "validation_param = rf[\"validation_param\"]\n",
    "\n",
    "validate_model(\n",
    "\tmodel=best_estimator, \n",
    "\tmodel_name=\"Random Forest\", \n",
    "\tparam_name=validation_param, \n",
    "\tparam_range=rf[\"hyperparameters\"][validation_param], \n",
    "\tX_train=X_train_filtered, \n",
    "\ty_train=y_train, \n",
    "\tX_test=X_test_filtered, \n",
    "\ty_test=y_test, \n",
    "\tkf=kf, \n",
    "\tscorer=rmse\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb9791-4327-4477-b427-a151438c80ad",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We're gonna make prediction about the revenue of movies present in `X2.csv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "841f872b124843c3cec9b84aa649cbdc5d28908a0e1b01e34eab5b6f0153b5f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
